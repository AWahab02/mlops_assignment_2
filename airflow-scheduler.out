[[34m2024-05-12T20:27:26.496+0500[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-05-12T20:27:26.497+0500[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2024-05-12T20:27:26.528+0500[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2024-05-12T20:27:26.529+0500[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[[34m2024-05-12T20:27:26.532+0500[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 11716[0m
[[34m2024-05-12T20:27:26.534+0500[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-12T20:27:26.536+0500[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-05-12T20:27:26.557+0500] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-05-12T20:30:04.283+0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_transform_load_data_dag.extract_data manual__2024-05-12T15:30:03.761985+00:00 [scheduled]>[0m
[[34m2024-05-12T20:30:04.283+0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_transform_load_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-12T20:30:04.283+0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_transform_load_data_dag.extract_data manual__2024-05-12T15:30:03.761985+00:00 [scheduled]>[0m
[[34m2024-05-12T20:30:04.285+0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_transform_load_data_dag', task_id='extract_data', run_id='manual__2024-05-12T15:30:03.761985+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-05-12T20:30:04.285+0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_transform_load_data_dag', 'extract_data', 'manual__2024-05-12T15:30:03.761985+00:00', '--local', '--subdir', '/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py'][0m
[[34m2024-05-12T20:30:04.302+0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_transform_load_data_dag', 'extract_data', 'manual__2024-05-12T15:30:03.761985+00:00', '--local', '--subdir', '/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py'][0m
[[34m2024-05-12T20:30:05.466+0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py[0m
[[34m2024-05-12T20:30:05.635+0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-12T20:30:05.868+0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-12T20:30:05.922+0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-05-12T20:30:05.931+0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-05-12T20:30:05.931+0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-05-12T20:30:06.052+0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-12T20:30:06.313+0500[0m] {[34mworkday.py:[0m41} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2024-05-12T20:30:06.345+0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_transform_load_data_dag.extract_data manual__2024-05-12T15:30:03.761985+00:00 [queued]> on host LAPTOP-DNNCGPVF.[0m
[[34m2024-05-12T20:30:27.025+0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_transform_load_data_dag', task_id='extract_data', run_id='manual__2024-05-12T15:30:03.761985+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-12T20:30:27.030+0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_transform_load_data_dag, task_id=extract_data, run_id=manual__2024-05-12T15:30:03.761985+00:00, map_index=-1, run_start_date=2024-05-12 15:30:06.401434+00:00, run_end_date=2024-05-12 15:30:21.500229+00:00, run_duration=15.098795, state=failed, executor_state=success, try_number=1, max_tries=1, job_id=3, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-05-12 15:30:04.284419+00:00, queued_by_job_id=2, pid=12752[0m
[[34m2024-05-12T20:30:29.509+0500[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_transform_load_data_dag @ 2024-05-12 15:30:03.761985+00:00: manual__2024-05-12T15:30:03.761985+00:00, state:running, queued_at: 2024-05-12 15:30:03.805513+00:00. externally triggered: True> failed[0m
[[34m2024-05-12T20:30:29.510+0500[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_transform_load_data_dag, execution_date=2024-05-12 15:30:03.761985+00:00, run_id=manual__2024-05-12T15:30:03.761985+00:00, run_start_date=2024-05-12 15:30:04.204563+00:00, run_end_date=2024-05-12 15:30:29.510404+00:00, run_duration=25.305841, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-05-12 15:30:03.761985+00:00, data_interval_end=2024-05-12 15:30:03.761985+00:00, dag_hash=5e688a83f4f070ff9a3ce5198a9303ec[0m
[[34m2024-05-12T20:32:22.190+0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_transform_load_data_dag.extract_data manual__2024-05-12T15:32:20.356671+00:00 [scheduled]>[0m
[[34m2024-05-12T20:32:22.191+0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_transform_load_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-12T20:32:22.191+0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_transform_load_data_dag.extract_data manual__2024-05-12T15:32:20.356671+00:00 [scheduled]>[0m
[[34m2024-05-12T20:32:22.194+0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_transform_load_data_dag', task_id='extract_data', run_id='manual__2024-05-12T15:32:20.356671+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-05-12T20:32:22.194+0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_transform_load_data_dag', 'extract_data', 'manual__2024-05-12T15:32:20.356671+00:00', '--local', '--subdir', '/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py'][0m
[[34m2024-05-12T20:32:22.210+0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_transform_load_data_dag', 'extract_data', 'manual__2024-05-12T15:32:20.356671+00:00', '--local', '--subdir', '/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py'][0m
[[34m2024-05-12T20:32:23.450+0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py[0m
[[34m2024-05-12T20:32:23.617+0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-12T20:32:23.811+0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-12T20:32:23.879+0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-05-12T20:32:23.887+0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-05-12T20:32:23.888+0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-05-12T20:32:23.979+0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-12T20:32:24.196+0500[0m] {[34mworkday.py:[0m41} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2024-05-12T20:32:24.221+0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_transform_load_data_dag.extract_data manual__2024-05-12T15:32:20.356671+00:00 [queued]> on host LAPTOP-DNNCGPVF.[0m
[[34m2024-05-12T20:33:28.177+0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_transform_load_data_dag', task_id='extract_data', run_id='manual__2024-05-12T15:32:20.356671+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-12T20:33:28.179+0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_transform_load_data_dag, task_id=extract_data, run_id=manual__2024-05-12T15:32:20.356671+00:00, map_index=-1, run_start_date=2024-05-12 15:32:24.270004+00:00, run_end_date=2024-05-12 15:33:27.791931+00:00, run_duration=63.521927, state=success, executor_state=success, try_number=1, max_tries=1, job_id=4, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-05-12 15:32:22.192906+00:00, queued_by_job_id=2, pid=13602[0m
[[34m2024-05-12T20:33:28.190+0500[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=11716) last sent a heartbeat 66.05 seconds ago! Restarting it[0m
[[34m2024-05-12T20:33:28.192+0500[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 11716. PIDs of all processes in the group: [11716][0m
[[34m2024-05-12T20:33:28.192+0500[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 11716[0m
[[34m2024-05-12T20:33:28.284+0500[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=11716, status='terminated', exitcode=0, started='20:27:25') (11716) terminated with exit code 0[0m
[[34m2024-05-12T20:33:28.289+0500[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 14165[0m
[[34m2024-05-12T20:33:28.295+0500[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-05-12T20:33:28.318+0500] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-05-12T20:33:28.343+0500[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-12T20:33:28.612+0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_transform_load_data_dag.transform_data manual__2024-05-12T15:32:20.356671+00:00 [scheduled]>[0m
[[34m2024-05-12T20:33:28.613+0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_transform_load_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-12T20:33:28.613+0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_transform_load_data_dag.transform_data manual__2024-05-12T15:32:20.356671+00:00 [scheduled]>[0m
[[34m2024-05-12T20:33:28.614+0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_transform_load_data_dag', task_id='transform_data', run_id='manual__2024-05-12T15:32:20.356671+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-12T20:33:28.615+0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_transform_load_data_dag', 'transform_data', 'manual__2024-05-12T15:32:20.356671+00:00', '--local', '--subdir', '/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py'][0m
[[34m2024-05-12T20:33:28.625+0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_transform_load_data_dag', 'transform_data', 'manual__2024-05-12T15:32:20.356671+00:00', '--local', '--subdir', '/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py'][0m
[[34m2024-05-12T20:33:29.817+0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py[0m
[[34m2024-05-12T20:33:30.008+0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-12T20:33:30.195+0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-12T20:33:30.243+0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-05-12T20:33:30.251+0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-05-12T20:33:30.251+0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-05-12T20:33:30.340+0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-12T20:33:30.541+0500[0m] {[34mworkday.py:[0m41} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2024-05-12T20:33:30.564+0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_transform_load_data_dag.transform_data manual__2024-05-12T15:32:20.356671+00:00 [queued]> on host LAPTOP-DNNCGPVF.[0m
[[34m2024-05-12T20:33:31.191+0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_transform_load_data_dag', task_id='transform_data', run_id='manual__2024-05-12T15:32:20.356671+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-12T20:33:31.194+0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_transform_load_data_dag, task_id=transform_data, run_id=manual__2024-05-12T15:32:20.356671+00:00, map_index=-1, run_start_date=2024-05-12 15:33:30.640088+00:00, run_end_date=2024-05-12 15:33:30.838462+00:00, run_duration=0.198374, state=success, executor_state=success, try_number=1, max_tries=1, job_id=5, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-12 15:33:28.613894+00:00, queued_by_job_id=2, pid=14193[0m
[[34m2024-05-12T20:33:31.352+0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_transform_load_data_dag.load_data manual__2024-05-12T15:32:20.356671+00:00 [scheduled]>[0m
[[34m2024-05-12T20:33:31.352+0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_transform_load_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-12T20:33:31.353+0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_transform_load_data_dag.load_data manual__2024-05-12T15:32:20.356671+00:00 [scheduled]>[0m
[[34m2024-05-12T20:33:31.354+0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_transform_load_data_dag', task_id='load_data', run_id='manual__2024-05-12T15:32:20.356671+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-05-12T20:33:31.354+0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_transform_load_data_dag', 'load_data', 'manual__2024-05-12T15:32:20.356671+00:00', '--local', '--subdir', '/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py'][0m
[[34m2024-05-12T20:33:31.372+0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_transform_load_data_dag', 'load_data', 'manual__2024-05-12T15:32:20.356671+00:00', '--local', '--subdir', '/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py'][0m
[[34m2024-05-12T20:33:32.408+0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py[0m
[[34m2024-05-12T20:33:32.564+0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-12T20:33:32.747+0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-12T20:33:32.800+0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-05-12T20:33:32.807+0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-05-12T20:33:32.807+0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-05-12T20:33:32.891+0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-12T20:33:33.072+0500[0m] {[34mworkday.py:[0m41} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2024-05-12T20:33:33.093+0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_transform_load_data_dag.load_data manual__2024-05-12T15:32:20.356671+00:00 [queued]> on host LAPTOP-DNNCGPVF.[0m

To track the changes with git, run:

	git add .gitignore transformed_data.csv.dvc

To enable auto staging, run:

	dvc config core.autostage true
[[34m2024-05-12T20:34:09.923+0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_transform_load_data_dag', task_id='load_data', run_id='manual__2024-05-12T15:32:20.356671+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-12T20:34:09.926+0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_transform_load_data_dag, task_id=load_data, run_id=manual__2024-05-12T15:32:20.356671+00:00, map_index=-1, run_start_date=2024-05-12 15:33:33.169274+00:00, run_end_date=2024-05-12 15:34:09.591521+00:00, run_duration=36.422247, state=success, executor_state=success, try_number=1, max_tries=1, job_id=6, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-05-12 15:33:31.353547+00:00, queued_by_job_id=2, pid=14218[0m
[[34m2024-05-12T20:34:10.084+0500[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun extract_transform_load_data_dag @ 2024-05-12 15:32:20.356671+00:00: manual__2024-05-12T15:32:20.356671+00:00, state:running, queued_at: 2024-05-12 15:32:20.375800+00:00. externally triggered: True> successful[0m
[[34m2024-05-12T20:34:10.084+0500[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_transform_load_data_dag, execution_date=2024-05-12 15:32:20.356671+00:00, run_id=manual__2024-05-12T15:32:20.356671+00:00, run_start_date=2024-05-12 15:32:22.151890+00:00, run_end_date=2024-05-12 15:34:10.084608+00:00, run_duration=107.932718, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-05-12 15:32:20.356671+00:00, data_interval_end=2024-05-12 15:32:20.356671+00:00, dag_hash=5e688a83f4f070ff9a3ce5198a9303ec[0m
[[34m2024-05-12T20:37:22.438+0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_transform_load_data_dag.extract_data manual__2024-05-12T15:37:21.017119+00:00 [scheduled]>[0m
[[34m2024-05-12T20:37:22.439+0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_transform_load_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-12T20:37:22.439+0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_transform_load_data_dag.extract_data manual__2024-05-12T15:37:21.017119+00:00 [scheduled]>[0m
[[34m2024-05-12T20:37:22.440+0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_transform_load_data_dag', task_id='extract_data', run_id='manual__2024-05-12T15:37:21.017119+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-05-12T20:37:22.440+0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_transform_load_data_dag', 'extract_data', 'manual__2024-05-12T15:37:21.017119+00:00', '--local', '--subdir', '/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py'][0m
[[34m2024-05-12T20:37:22.453+0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_transform_load_data_dag', 'extract_data', 'manual__2024-05-12T15:37:21.017119+00:00', '--local', '--subdir', '/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py'][0m
[[34m2024-05-12T20:37:23.815+0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py[0m
[[34m2024-05-12T20:37:24.062+0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-12T20:37:24.291+0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-12T20:37:24.341+0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-05-12T20:37:24.350+0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-05-12T20:37:24.351+0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-05-12T20:37:24.435+0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-12T20:37:24.634+0500[0m] {[34mworkday.py:[0m41} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2024-05-12T20:37:24.657+0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_transform_load_data_dag.extract_data manual__2024-05-12T15:37:21.017119+00:00 [queued]> on host LAPTOP-DNNCGPVF.[0m
[[34m2024-05-12T20:38:27.028+0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_transform_load_data_dag', task_id='extract_data', run_id='manual__2024-05-12T15:37:21.017119+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-12T20:38:27.030+0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_transform_load_data_dag, task_id=extract_data, run_id=manual__2024-05-12T15:37:21.017119+00:00, map_index=-1, run_start_date=2024-05-12 15:37:24.704826+00:00, run_end_date=2024-05-12 15:38:26.589907+00:00, run_duration=61.885081, state=success, executor_state=success, try_number=1, max_tries=1, job_id=7, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-05-12 15:37:22.439710+00:00, queued_by_job_id=2, pid=15480[0m
[[34m2024-05-12T20:38:27.041+0500[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=14165) last sent a heartbeat 64.65 seconds ago! Restarting it[0m
[[34m2024-05-12T20:38:27.043+0500[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 14165. PIDs of all processes in the group: [14165][0m
[[34m2024-05-12T20:38:27.043+0500[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 14165[0m
[[34m2024-05-12T20:38:27.136+0500[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=14165, status='terminated', exitcode=0, started='20:33:27') (14165) terminated with exit code 0[0m
[[34m2024-05-12T20:38:27.141+0500[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 16001[0m
[[34m2024-05-12T20:38:27.147+0500[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-05-12T20:38:27.171+0500] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-05-12T20:38:27.452+0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_transform_load_data_dag.transform_data manual__2024-05-12T15:37:21.017119+00:00 [scheduled]>[0m
[[34m2024-05-12T20:38:27.452+0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_transform_load_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-12T20:38:27.452+0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_transform_load_data_dag.transform_data manual__2024-05-12T15:37:21.017119+00:00 [scheduled]>[0m
[[34m2024-05-12T20:38:27.454+0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_transform_load_data_dag', task_id='transform_data', run_id='manual__2024-05-12T15:37:21.017119+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-12T20:38:27.454+0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_transform_load_data_dag', 'transform_data', 'manual__2024-05-12T15:37:21.017119+00:00', '--local', '--subdir', '/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py'][0m
[[34m2024-05-12T20:38:27.465+0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_transform_load_data_dag', 'transform_data', 'manual__2024-05-12T15:37:21.017119+00:00', '--local', '--subdir', '/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py'][0m
[[34m2024-05-12T20:38:28.472+0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py[0m
[[34m2024-05-12T20:38:28.622+0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-12T20:38:28.802+0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-12T20:38:28.847+0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-05-12T20:38:28.854+0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-05-12T20:38:28.854+0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-05-12T20:38:28.936+0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-12T20:38:29.116+0500[0m] {[34mworkday.py:[0m41} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2024-05-12T20:38:29.139+0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_transform_load_data_dag.transform_data manual__2024-05-12T15:37:21.017119+00:00 [queued]> on host LAPTOP-DNNCGPVF.[0m
[[34m2024-05-12T20:38:29.771+0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_transform_load_data_dag', task_id='transform_data', run_id='manual__2024-05-12T15:37:21.017119+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-12T20:38:29.774+0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_transform_load_data_dag, task_id=transform_data, run_id=manual__2024-05-12T15:37:21.017119+00:00, map_index=-1, run_start_date=2024-05-12 15:38:29.228843+00:00, run_end_date=2024-05-12 15:38:29.447100+00:00, run_duration=0.218257, state=success, executor_state=success, try_number=1, max_tries=1, job_id=8, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-12 15:38:27.453255+00:00, queued_by_job_id=2, pid=16010[0m
[[34m2024-05-12T20:38:29.785+0500[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-12T20:38:29.943+0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_transform_load_data_dag.load_data manual__2024-05-12T15:37:21.017119+00:00 [scheduled]>[0m
[[34m2024-05-12T20:38:29.944+0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_transform_load_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-12T20:38:29.944+0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_transform_load_data_dag.load_data manual__2024-05-12T15:37:21.017119+00:00 [scheduled]>[0m
[[34m2024-05-12T20:38:29.945+0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_transform_load_data_dag', task_id='load_data', run_id='manual__2024-05-12T15:37:21.017119+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-05-12T20:38:29.945+0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_transform_load_data_dag', 'load_data', 'manual__2024-05-12T15:37:21.017119+00:00', '--local', '--subdir', '/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py'][0m
[[34m2024-05-12T20:38:29.956+0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_transform_load_data_dag', 'load_data', 'manual__2024-05-12T15:37:21.017119+00:00', '--local', '--subdir', '/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py'][0m
[[34m2024-05-12T20:38:30.883+0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py[0m
[[34m2024-05-12T20:38:31.030+0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-12T20:38:31.203+0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-12T20:38:31.253+0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-05-12T20:38:31.260+0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-05-12T20:38:31.260+0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-05-12T20:38:31.343+0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-12T20:38:31.525+0500[0m] {[34mworkday.py:[0m41} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2024-05-12T20:38:31.548+0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_transform_load_data_dag.load_data manual__2024-05-12T15:37:21.017119+00:00 [queued]> on host LAPTOP-DNNCGPVF.[0m
Initialized DVC repository.

You can now commit the changes to git.

+---------------------------------------------------------------------+
|                                                                     |
|        DVC has enabled anonymous aggregate usage analytics.         |
|     Read the analytics documentation (and how to opt-out) here:     |
|             <https://dvc.org/doc/user-guide/analytics>              |
|                                                                     |
+---------------------------------------------------------------------+

What's next?
------------
- Check out the documentation: <https://dvc.org/doc>
- Get help and share ideas: <https://dvc.org/chat>
- Star us on GitHub: <https://github.com/iterative/dvc>

To track the changes with git, run:

	git add transformed_data.csv.dvc

To enable auto staging, run:

	dvc config core.autostage true
1 file pushed
[[34m2024-05-12T20:38:43.475+0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_transform_load_data_dag', task_id='load_data', run_id='manual__2024-05-12T15:37:21.017119+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-12T20:38:43.478+0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_transform_load_data_dag, task_id=load_data, run_id=manual__2024-05-12T15:37:21.017119+00:00, map_index=-1, run_start_date=2024-05-12 15:38:31.638652+00:00, run_end_date=2024-05-12 15:38:43.138290+00:00, run_duration=11.499638, state=success, executor_state=success, try_number=1, max_tries=1, job_id=9, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-05-12 15:38:29.944665+00:00, queued_by_job_id=2, pid=16049[0m
[[34m2024-05-12T20:38:43.681+0500[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun extract_transform_load_data_dag @ 2024-05-12 15:37:21.017119+00:00: manual__2024-05-12T15:37:21.017119+00:00, state:running, queued_at: 2024-05-12 15:37:21.039283+00:00. externally triggered: True> successful[0m
[[34m2024-05-12T20:38:43.681+0500[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_transform_load_data_dag, execution_date=2024-05-12 15:37:21.017119+00:00, run_id=manual__2024-05-12T15:37:21.017119+00:00, run_start_date=2024-05-12 15:37:22.399947+00:00, run_end_date=2024-05-12 15:38:43.681799+00:00, run_duration=81.281852, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-05-12 15:37:21.017119+00:00, data_interval_end=2024-05-12 15:37:21.017119+00:00, dag_hash=5e688a83f4f070ff9a3ce5198a9303ec[0m
[[34m2024-05-12T20:43:29.920+0500[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-12T20:44:12.496+0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_transform_load_data_dag.extract_data manual__2024-05-12T15:44:11.738292+00:00 [scheduled]>[0m
[[34m2024-05-12T20:44:12.497+0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_transform_load_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-12T20:44:12.497+0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_transform_load_data_dag.extract_data manual__2024-05-12T15:44:11.738292+00:00 [scheduled]>[0m
[[34m2024-05-12T20:44:12.498+0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_transform_load_data_dag', task_id='extract_data', run_id='manual__2024-05-12T15:44:11.738292+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-05-12T20:44:12.499+0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_transform_load_data_dag', 'extract_data', 'manual__2024-05-12T15:44:11.738292+00:00', '--local', '--subdir', '/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py'][0m
[[34m2024-05-12T20:44:12.512+0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_transform_load_data_dag', 'extract_data', 'manual__2024-05-12T15:44:11.738292+00:00', '--local', '--subdir', '/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py'][0m
[[34m2024-05-12T20:44:13.675+0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py[0m
[[34m2024-05-12T20:44:13.859+0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-12T20:44:14.092+0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-12T20:44:14.144+0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-05-12T20:44:14.152+0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-05-12T20:44:14.153+0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-05-12T20:44:14.231+0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-12T20:44:14.518+0500[0m] {[34mworkday.py:[0m41} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2024-05-12T20:44:14.543+0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_transform_load_data_dag.extract_data manual__2024-05-12T15:44:11.738292+00:00 [queued]> on host LAPTOP-DNNCGPVF.[0m
[[34m2024-05-12T20:45:18.427+0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_transform_load_data_dag', task_id='extract_data', run_id='manual__2024-05-12T15:44:11.738292+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-12T20:45:18.430+0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_transform_load_data_dag, task_id=extract_data, run_id=manual__2024-05-12T15:44:11.738292+00:00, map_index=-1, run_start_date=2024-05-12 15:44:14.592783+00:00, run_end_date=2024-05-12 15:45:18.009002+00:00, run_duration=63.416219, state=success, executor_state=success, try_number=1, max_tries=1, job_id=10, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-05-12 15:44:12.497790+00:00, queued_by_job_id=2, pid=18347[0m
[[34m2024-05-12T20:45:18.441+0500[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=16001) last sent a heartbeat 65.99 seconds ago! Restarting it[0m
[[34m2024-05-12T20:45:18.444+0500[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 16001. PIDs of all processes in the group: [16001][0m
[[34m2024-05-12T20:45:18.444+0500[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 16001[0m
[[34m2024-05-12T20:45:18.537+0500[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=16001, status='terminated', exitcode=0, started='20:38:26') (16001) terminated with exit code 0[0m
[[34m2024-05-12T20:45:18.539+0500[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 18841[0m
[[34m2024-05-12T20:45:18.544+0500[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-05-12T20:45:18.567+0500] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-05-12T20:45:18.856+0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_transform_load_data_dag.transform_data manual__2024-05-12T15:44:11.738292+00:00 [scheduled]>[0m
[[34m2024-05-12T20:45:18.856+0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_transform_load_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-12T20:45:18.856+0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_transform_load_data_dag.transform_data manual__2024-05-12T15:44:11.738292+00:00 [scheduled]>[0m
[[34m2024-05-12T20:45:18.858+0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_transform_load_data_dag', task_id='transform_data', run_id='manual__2024-05-12T15:44:11.738292+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-05-12T20:45:18.858+0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_transform_load_data_dag', 'transform_data', 'manual__2024-05-12T15:44:11.738292+00:00', '--local', '--subdir', '/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py'][0m
[[34m2024-05-12T20:45:18.869+0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_transform_load_data_dag', 'transform_data', 'manual__2024-05-12T15:44:11.738292+00:00', '--local', '--subdir', '/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py'][0m
[[34m2024-05-12T20:45:19.997+0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py[0m
[[34m2024-05-12T20:45:20.145+0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-12T20:45:20.321+0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-12T20:45:20.367+0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-05-12T20:45:20.375+0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-05-12T20:45:20.376+0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-05-12T20:45:20.464+0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-12T20:45:20.670+0500[0m] {[34mworkday.py:[0m41} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2024-05-12T20:45:20.695+0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_transform_load_data_dag.transform_data manual__2024-05-12T15:44:11.738292+00:00 [queued]> on host LAPTOP-DNNCGPVF.[0m
[[34m2024-05-12T20:45:21.319+0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_transform_load_data_dag', task_id='transform_data', run_id='manual__2024-05-12T15:44:11.738292+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-12T20:45:21.323+0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_transform_load_data_dag, task_id=transform_data, run_id=manual__2024-05-12T15:44:11.738292+00:00, map_index=-1, run_start_date=2024-05-12 15:45:20.756199+00:00, run_end_date=2024-05-12 15:45:20.953517+00:00, run_duration=0.197318, state=success, executor_state=success, try_number=1, max_tries=1, job_id=11, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-05-12 15:45:18.856968+00:00, queued_by_job_id=2, pid=18851[0m
[[34m2024-05-12T20:45:21.584+0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_transform_load_data_dag.load_data manual__2024-05-12T15:44:11.738292+00:00 [scheduled]>[0m
[[34m2024-05-12T20:45:21.585+0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_transform_load_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-12T20:45:21.585+0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_transform_load_data_dag.load_data manual__2024-05-12T15:44:11.738292+00:00 [scheduled]>[0m
[[34m2024-05-12T20:45:21.586+0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_transform_load_data_dag', task_id='load_data', run_id='manual__2024-05-12T15:44:11.738292+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-05-12T20:45:21.586+0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_transform_load_data_dag', 'load_data', 'manual__2024-05-12T15:44:11.738292+00:00', '--local', '--subdir', '/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py'][0m
[[34m2024-05-12T20:45:21.598+0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_transform_load_data_dag', 'load_data', 'manual__2024-05-12T15:44:11.738292+00:00', '--local', '--subdir', '/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py'][0m
[[34m2024-05-12T20:45:22.663+0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/dagfile.py[0m
[[34m2024-05-12T20:45:22.841+0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-12T20:45:23.042+0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-12T20:45:23.089+0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-05-12T20:45:23.096+0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/awahab02/mlops_assignment_2/myenv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-05-12T20:45:23.096+0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-05-12T20:45:23.177+0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-12T20:45:23.376+0500[0m] {[34mworkday.py:[0m41} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2024-05-12T20:45:23.406+0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_transform_load_data_dag.load_data manual__2024-05-12T15:44:11.738292+00:00 [queued]> on host LAPTOP-DNNCGPVF.[0m
Initialized DVC repository.

You can now commit the changes to git.

+---------------------------------------------------------------------+
|                                                                     |
|        DVC has enabled anonymous aggregate usage analytics.         |
|     Read the analytics documentation (and how to opt-out) here:     |
|             <https://dvc.org/doc/user-guide/analytics>              |
|                                                                     |
+---------------------------------------------------------------------+

What's next?
------------
- Check out the documentation: <https://dvc.org/doc>
- Get help and share ideas: <https://dvc.org/chat>
- Star us on GitHub: <https://github.com/iterative/dvc>

To track the changes with git, run:

	git add transformed_data.csv.dvc

To enable auto staging, run:

	dvc config core.autostage true
1 file pushed
